# -*- coding: utf-8 -*-
"""Recommendation System Model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jtFr9t18IaVxS8lzG9OtDM5EEhgFJnwL

# Laporan Proyek Machine Learning - Zefanya Danovanta Tarigan

## Project Overview

Buku adalah kumpulan lembaran atau kertas yang mengandung tulisan, gambar, atau tempelan, yang dijilid pada salah satu sisinya. Bahan-bahan pembentuk buku bisa berupa kertas, kayu, bahkan gading gajah. Setiap sisi lembaran dalam buku disebut halaman. Seiring kemajuan teknologi, hadir pula buku elektronik (e-book) yang dapat diakses menggunakan perangkat seperti komputer, tablet, atau ponsel dengan perangkat lunak khusus .

Sistem rekomendasi digunakan untuk memprediksi atau mengidentifikasi barang, termasuk buku, yang sesuai dengan minat pengguna. Dengan kemajuan teknologi, tersedia berbagai platform yang memudahkan pengguna mencari referensi buku, baik melalui aplikasi digital maupun secara langsung.

Jumlah buku yang sangat banyak sering kali membuat pembaca kesulitan menentukan pilihan bacaan berikutnya. Beberapa pembaca lebih suka buku dengan reputasi penjualan tinggi, sementara yang lain mencari buku serupa dengan yang pernah mereka baca. Ada juga pembaca yang memilih buku berdasarkan rating atau ulasan yang tersedia.

## Alasan Pembuatan Proyek dan Referensi Terkait Proyek :
- Pengembangan proyek sistem rekomendasi ini memiliki peran yang sangat penting. Tujuannya adalah memberikan solusi atas kesulitan pengguna dalam memilih buku yang belum pernah mereka baca. Oleh karena itu, dibutuhkan sebuah sistem yang dapat merekomendasikan buku-buku menarik yang sesuai dengan minat pengguna. Ketika pengguna merasa puas dengan fitur-fitur yang tersedia di dalam platform, mereka cenderung akan terus menggunakan dan berlangganan layanan yang disediakan .

## Data Understanding

Dataset yang digunakan dalam proyek machine learning ini adalah Book Recommendation Dataset, yang terdiri dari 271.360 data buku, 278.858 data pengguna, dan 1.149.780 data penilaian. Dataset ini bersifat open-source dan dipublikasikan oleh MÃ–BIUS melalui platform Kaggle. Dataset ini mencakup tiga kategori utama: buku (format CSV), penilaian/rating (format CSV), dan pengguna (format CSV).

**Tautan Dataset :** [Book Recommendation Dataset](https://www.kaggle.com/datasets/arashnic/book-recommendation-dataset)

Dataset yang pertama adalah **Users.csv** memiliki jumlah 278858 data dan 3 kolom, yakni :
1. `User-ID` : ID pengguna dari toko buku online. **Tipe data =** `Int64`
2. `Location`: lokasi pengguna. **Tipe data =** `Object`
3. `Age` : usia pengguna. **Tipe data =** `Float64`

Dataset yang kedua yakni 'Books' yang memiliki jumlah 278858 data dan memiliki 8 kolom, diantaranya :
1. `ISBN`: identifikasi dari masing-masing buku. **Tipe data =** `Object`
2. `Book-Title`: judul buku. **Tipe data =** `Object`
3. `Book-Author`: penulis buku. **Tipe data =** `Object`
4. `Year-Of-Publication`: tahun dipublikasikannya buku. **Tipe data =** `Object`
5. `Publisher`: penerbit buku. **Tipe data =** `Object`
6. `Image-URL-S`: URL gambar cover buku dalam ukuran S(Small) **Tipe data =** `Object`
7. `Image-URL-M`: URL gambar cover buku dalam ukuran M(Medium) **Tipe data =** `Object`
8. `Image-URL-L`: URL gambar cover buku dalam ukuran L(Large) **Tipe data =** `Object`

Dataset yang ketiga yakni 'Ratings' yang memiliki jumlah 1149780 data dan memiliki 3 kolom, berikut penjelasan mengenai kolom-kolomnya :
Ratings.csv

1. `User-ID`: ID dari user yang memberikan rating terhadap buku. **Tipe data =** `Int64`
2. `Kolom 'ISBN`: identifikasi buku atau nomor buku yang diberi rating oleh user **Tipe data =** `Object`
3. `Book-Rating`: nilai Rating dari buku, skala yang ada dalam rating ini yakni dari 0-10. **Tipe data =** `Int64`

## Data Loading

### Menyiapkan Kredensial Kaggle
"""

# upload kaggle.json
from google.colab import files
files.upload()

!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json
!ls ~/.kaggle

"""### Data Loading"""

# download dataset, choose 'copy api command' from kaggle dataset
!kaggle datasets download -d arashnic/book-recommendation-dataset

# unzip
!mkdir book-recommendation-dataset.zip
!unzip book-recommendation-dataset.zip -d book-recommendation-dataset
!ls book-recommendation-dataset

"""### Import Library"""

# Commented out IPython magic to ensure Python compatibility.
# Import library
import pandas as pd
import numpy as np
from zipfile import ZipFile
import tensorflow as tf
import seaborn as sns
import matplotlib.pyplot as plt
import keras
# %matplotlib inline
import seaborn as sns

from tensorflow import keras
from tensorflow.keras import layers
from pathlib import Path
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.metrics import precision_score
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from zipfile import ZipFile
from numpy import sqrt
from sklearn.metrics import mean_squared_error

# memuat file ke dalam variable
Books = pd.read_csv("/content/book-recommendation-dataset/Books.csv")
Ratings = pd.read_csv("/content/book-recommendation-dataset/Ratings.csv")
Users = pd.read_csv("/content/book-recommendation-dataset/Users.csv")

"""## Exploratory Data Analysis

Pada data loading, telah dideklarasikan variabel yang akan dipakai. Variable tersebut diantaranya :

- `Books` : Buku diidentifikasi dengan ISBN masing-masing.
- `Ratings` : Berisi informasi penilaian buku
- `Users` : Berisi pengguna.
"""

# menampilkan jumlah shape pada masing-masing file

print("Books Shape:", Books.shape)
print("Users Shape:", Users.shape)
print("Ratings Shape:", Ratings.shape)

"""Dari perintah diatas, dapat memberikan informasi dimana dataset `Books` memiliki 271360 baris dan 8 kolom, sedangkan Dataset `Users` memiliki 278858 baris dan 3 kolom, lalu Dataset `Ratings` memiliki 1149780 baris dan 3 kolom

### Variabel `Books`
"""

Books.head()

#Cek informasi dari data Books
Books.info()
print('Jumlah Duplikasi : ',Books.duplicated().sum())

Books.isnull().sum()

"""Dataset yang Pertama yakni `Books` memiliki memiliki 8 kolom, diantaranya :
1. `ISBN`: identifikasi dari masing-masing buku. **Tipe data =** `Object`
2. `Book-Title`: judul buku. **Tipe data =** `Object`
3. `Book-Author`: penulis buku. **Tipe data =** `Object`
4. `Year-Of-Publication`: tahun dipublikasikannya buku. **Tipe data =** `Object`
5. `Publisher`: penerbit buku. **Tipe data =** `Object`
6. `Image-URL-S`: URL gambar cover buku dalam ukuran S(Small) **Tipe data =** `Object`
7. `Image-URL-M`: URL gambar cover buku dalam ukuran M(Medium) **Tipe data =** `Object`
8. `Image-URL-L`: URL gambar cover buku dalam ukuran L(Large) **Tipe data =** `Object`

lalu pada masing masing kolom tidak memiliki duplikasi data

"""

print('Banyak Buku :', len(Books))

"""### Variabel `Users`"""

Users.head()

Users.info()
print('Jumlah Duplikasi : ',Users.duplicated().sum())

Users.isnull().sum()

"""Dataset yang Keuda adalah **Users** memiliki 3 kolom, yakni :
1. `User-ID` : ID pengguna dari toko buku online. **Tipe data =** `Int64`
2. `Location`: lokasi pengguna. **Tipe data =** `Object`
3. `Age` : usia pengguna. **Tipe data =** `Float64`

lalu tidak memiliki duplikasi data di masing masing kolom
"""

print('Banyak Pengguna :', len(Users))

"""### Variabel `Ratings`"""

Ratings.head()

Ratings.info()
print('Jumlah Duplikasi : ',Ratings.duplicated().sum())

Ratings.isnull().sum()

print('Banyak Rating :', len(Ratings))

"""## Data Preparation

Data preparation bertujuan untuk menyiapkan data sebelum masuk ke proses modeling. Selain itu, data preparation juga berguna untuk meningkatkan akurasi saat training data. Pada dataset ini, yang akan kita lakukan yaitu menggabungkan dataset dengan fungsi merge() dan key ISBN, menghapus missing value serta menurut dataset berdasarkan ISBN serta menghapus hasil duplikat. Karena ketiga dataset yang digunakan merupakan dataset dalam jumlah yang banyak yaitu ada lebih dari 200.000, maka di pada proses ini hanya mengambil 12.000 data pertama dari setiap variabel di atas dalam pembuatan sistem rekomendasi ini dan menjadi variabel baru yaitu `data_users`, `data_books` dan `data_rating`
"""

data_users = Users.iloc[:12000]
print('Jumlah pengguna yang di gunakan :', len(data_users))

data_books = Books.iloc[:12000]
print('Jumlah Buku yang di gunakan :', len(data_books))

data_rating = Ratings.iloc[:12000]
print('Jumlah pengguna yang di gunakan :', len(data_rating))

"""### Variabel `data_books`

Pada dataset **data_books** kolom `Image-URL-S`, `Image-URL-M` dan `Image-URL-L` akan dihapus karena kolom tersebut tidak akan dipakai

Pada dataset **data_books** ada beberapa kolom akan diubah nama yaitu :
  - `Book-Title` = `Book_Title`
  - `Book-Author` = `Book_Author`
  - `Year-Of-Publication` = `Year_Publication`
"""

data_books.drop(['Image-URL-S', 'Image-URL-M', 'Image-URL-L' ], axis=1, inplace=True)
data_books.head()

data_books.rename(columns = {'Book-Title':'Book_Title', 'Book-Author': 'Book_Author', 'Year-Of-Publication': 'Year_Publication'},inplace = True)
data_books.head()

data_books['Year_Publication'] = pd.to_numeric(data_books['Year_Publication'], errors='coerce')

data_books.info()
print('Jumlah Duplikasi : ',data_books.duplicated().sum())

"""### Variabel `data_rating`

Pada dataset **data_rating** ada beberapa kolom akan diubah nama yaitu :
- `User-ID` = `UserID`
- `Book-Rating` = `Book_Rating`
"""

data_rating.rename(columns = {'User-ID':'UserID', 'Book-Rating': 'Book_Rating'},inplace = True)
data_rating.head()

"""### Variabel `data_users`

Pada dataset **data_users** kolom `Age` akan dihapus karena kolom tersebut akan dihapus

Pada dataset **data_users** ada kolom akan diubah nama yaitu :
- `User-ID` = `UserID`
"""

data_users.drop(['Age'], axis=1, inplace=True)
data_users.head()

data_users.rename(columns = {'User-ID':'UserID'}, inplace = True)
data_users.head()

print('Jumlah Buku berdasarkan Rating : ', len(data_rating.ISBN.unique()))
print('Jumlah Buku berdasarkan Daftar Buku : ', len(data_books.ISBN.unique()))
print('Jumlah Pengguna berdasarkan ID PEngguna : ', len(data_users.UserID.unique()))

"""**Menggabungkan `data_books` dan `data_rating` dengan menggunakan fungsi merge**"""

data_train = data_rating.merge(data_books, left_on = 'ISBN', right_on = 'ISBN')
data_train.head()

data_train.info()
print('Jumlah Duplikasi : ',data_train.duplicated().sum())

year=data_train['Year_Publication'].value_counts()[0:10]
plt.figure(figsize=(10,6))
plt.title("10 Tahun terbanyak publikasi", fontsize=20)
sns.barplot(x=year.index,y=year, palette = 'viridis')
plt.xlabel('Year',fontsize=14)
plt.ylabel('Count', fontsize=14)
plt.show()

"""**Menggabungkan `data_user` dan `data_rating` dengan menggunakan fungsi merge**"""

data_using = data_rating.merge(data_users, left_on = 'UserID', right_on = 'UserID')
data_using.head()

data_using.info()
print('Jumlah Duplikasi : ',data_using.duplicated().sum())

rating_counter=data_using['Book_Rating'].value_counts()[0:10]
plt.figure(figsize=(10,6))
plt.title("Jumlah Rating Buku yang Diberikan Pengguna", fontsize=20)
sns.barplot(x=rating_counter.index, y=rating_counter, palette='viridis')
plt.xlabel('Book_Rating', fontsize=14)
plt.ylabel('Jumlah Buku', fontsize=14)
plt.show()

most_author = data_train.Book_Author.value_counts().reset_index()
most_author.columns = ['Book_Author','count']

plt.figure(figsize = (10,6))
plt.title("10 Penulis Terpopuler",fontsize = 20)
sns.barplot(x = 'count', y = 'Book_Author', data = most_author.head(10), palette='viridis');
plt.ylabel('Book_Author', fontsize=14)
plt.xlabel('Count', fontsize=14)
plt.show()

most_loc = data_using.Location.value_counts().reset_index()
most_loc.columns = ['Location','count']

plt.figure(figsize = (10,6))
plt.title("Lokasi Penulis Terpopuler", fontsize=20)
sns.barplot(x = 'count', y = 'Location', data = most_loc.head(10), palette='viridis');
plt.ylabel('Location', fontsize=14)
plt.xlabel('Count', fontsize=14)
plt.show()

most_publis = data_train.Publisher.value_counts().reset_index()
most_publis.columns = ['Publisher','count']

plt.figure(figsize = (10,6))
plt.title("Publisher terbaik", fontsize= 20)
sns.barplot(x = 'count', y = 'Publisher', data = most_publis.head(10),  palette='viridis');
plt.ylabel('Publisher', fontsize=14)
plt.xlabel('Count', fontsize = 14)
plt.show()

data_aver = data_train.groupby('Book_Title', as_index=False)['Book_Rating'].mean()
temp = data_train.Book_Title.value_counts().reset_index()
temp.columns = ['Book_Title','count']
most_rated_by_reads = pd.merge(data_aver,temp,on='Book_Title')

most_rated_by_reads = most_rated_by_reads.sort_values('count',ascending=False)

plt.figure(figsize=(10,6))
plt.title("Rata-rata rating dengan buku terbanyak dibaca")
sns.barplot(x = 'Book_Rating', y = 'Book_Title', data = most_rated_by_reads.head(10), palette='viridis')
plt.xlabel('Book_Rating', fontsize=14)
plt.ylabel('Book_Title', fontsize=14)
plt.show()

"""## Data Cleaning

proses mendeteksi dan mengoreksi (atau menghapus) catatan yang rusak atau tidak akurat dari kumpulan catatan, tabel, atau basis data dan mengacu pada pengidentifikasian bagian data yang tidak lengkap, salah, tidak akurat atau tidak relevan dan kemudian mengganti, memodifikasi, atau menghapus data kotor atau kasar.
"""

# check missing values pada data_books
(data_books.isnull() | data_books.empty | data_books.isna()).sum()

# check missing values pada data_rating
(data_rating.isnull() | data_rating.empty | data_rating.isna()).sum()

# check missing values pada data_users
(data_users.isnull() | data_users.empty | data_users.isna()).sum()

# check missing values pada data_train
(data_train.isnull() | data_train.empty | data_train.isna()).sum()

# check missing values pada data_using
(data_using.isnull() | data_using.empty | data_using.isna()).sum()

"""Dari output diatas, dapat diambil kesimpulan bahwa masing masing kolom di semua dataset tidak mempuyai nilai yang kosong (Missing Value)


"""

data_prep = data_train
data_prep.sort_values('ISBN').head()

data_prep = data_prep.drop_duplicates('ISBN')
data_prep.head()

data_prus = data_using
data_prus.sort_values('UserID').head()

data_prus = data_prus.drop_duplicates('UserID')
data_prus.head()

# Mengonversi data series 'ISBNâ€™ menjadi dalam bentuk list
books_id = data_prep['ISBN'].tolist()

# Mengonversi data series â€˜Titleâ€™ menjadi dalam bentuk list
books_title = data_prep['Book_Title'].tolist()

# Mengonversi data series â€˜Authorâ€™ menjadi dalam bentuk list
books_author = data_prep['Book_Author'].tolist()

print('Jumlah ID Buku : ', len(books_id))
print('Jumlah Judul Buku : ', len(books_title))
print('Jumlah Penulis Buku : ', len(books_author))

# Membuat dictionary untuk data â€˜books_idâ€™, â€˜books_titleâ€™, dan â€˜books_authorâ€™
books_new = pd.DataFrame({
    'id': books_id,
    'title':books_title,
    'author': books_author
})
books_new.head()

data = data_rating
data.head()

# Mengubah UserID menjadi list tanpa nilai yang sama
user_ids = data['UserID'].unique().tolist()
print('list UserID: ', user_ids)

# Melakukan encoding userID
user_to_user_encoded = {x: i for i, x in enumerate(user_ids)}
print('encoded UserID : ', user_to_user_encoded)

# Melakukan proses encoding angka ke UserID
user_encoded_to_user = {i: x for i, x in enumerate(user_ids)}
print('encoded angka ke UserID: ', user_encoded_to_user)

# Mengubah ISBN menjadi list tanpa nilai yang sama
book_ids = data['ISBN'].unique().tolist()

# Melakukan proses encoding ISBN
book_to_book_encoded = {x: i for i, x in enumerate(book_ids)}

# Melakukan proses encoding angka ke ISBN
book_encoded_to_book = {i: x for i, x in enumerate(book_ids)}

#Selanjutnya, petakan userID dan ISBN ke dataframe yang berkaitan.

# Mapping userID ke dataframe user
data['user'] = data['UserID'].map(user_to_user_encoded)

# Mapping ISBN ke dataframe book
data['book'] = data['ISBN'].map(book_to_book_encoded)

# Mendapatkan jumlah user
num_users = len(user_to_user_encoded)
print(num_users)

# Mendapatkan jumlah book
num_book = len(book_encoded_to_book)
print(num_book)

# Mengubah rating menjadi nilai float
data['Book_Rating'] = data['Book_Rating'].values.astype(np.float32)

# Nilai minimum rating
min_rating = min(data['Book_Rating'])

# Nilai maksimal rating
max_rating = max(data['Book_Rating'])

print('Number of User: {}, Number of Resto: {}, Min Rating: {}, Max Rating: {}'.format(
    num_users, num_book, min_rating, max_rating
))

# Mengacak dataset
data = data.sample(frac=1, random_state=42)
data.head()

# Membuat variabel x untuk mencocokkan data user dan book menjadi satu value
x = data[['user', 'book']].values

# Membuat variabel y untuk membuat rating dari hasil
y = data['Book_Rating'].apply(lambda x: (x - min_rating) / (max_rating - min_rating)).values

# Membagi menjadi 90% data train dan 10% data validasi
train_indices = int(0.9 * data.shape[0])
x_train, x_val, y_train, y_val = (
    x[:train_indices],
    x[train_indices:],
    y[:train_indices],
    y[train_indices:]
)

print(x, y)

"""## Data Modeling

Proses untuk menjalankan algoritma Machine Learning untuk mengolah dataset yang sudah dibagi menjadi data training dan mengoptimalkan algoritma untuk menemukan pola atau output tertentu. Pada tahap ini saya menggunakan model collaborative filtering dimana menggunakan metode deep learning yang bertujuan menghasilkan rekomendasi buku.
"""

class RecommenderNet(tf.keras.Model):

  # Insialisasi fungsi
  def __init__(self, num_users, num_book, embedding_size, **kwargs):
    super(RecommenderNet, self).__init__(**kwargs)
    self.num_users = num_users
    self.num_book = num_book
    self.embedding_size = embedding_size
    self.user_embedding = layers.Embedding( # layer embedding user
        num_users,
        embedding_size,
        embeddings_initializer = 'he_normal',
        embeddings_regularizer = keras.regularizers.l2(1e-6)
    )
    self.user_bias = layers.Embedding(num_users, 1) # layer embedding user bias
    self.book_embedding = layers.Embedding( # layer embeddings book
        num_book,
        embedding_size,
        embeddings_initializer = 'he_normal',
        embeddings_regularizer = keras.regularizers.l2(1e-6)
    )
    self.book_bias = layers.Embedding(num_book, 1) # layer embedding book bias

  def call(self, inputs):
    user_vector = self.user_embedding(inputs[:,0]) # memanggil layer embedding 1
    user_bias = self.user_bias(inputs[:, 0]) # memanggil layer embedding 2
    book_vector = self.book_embedding(inputs[:, 1]) # memanggil layer embedding 3
    book_bias = self.book_bias(inputs[:, 1]) # memanggil layer embedding 4

    dot_user_book = tf.tensordot(user_vector, book_vector, 2)

    x = dot_user_book + user_bias + book_bias

    return tf.nn.sigmoid(x) # activation sigmoid

model = RecommenderNet(num_users, num_book, 50) # inisialisasi model

# model compile
model.compile(
    loss = tf.keras.losses.BinaryCrossentropy(),
    optimizer = keras.optimizers.Adam(learning_rate=0.001),
    metrics=[tf.keras.metrics.RootMeanSquaredError()]
)

# Memulai training

history = model.fit(
    x = x_train,
    y = y_train,
    batch_size = 64,
    epochs = 100,
    validation_data = (x_val, y_val)
)

"""## Evaluasi

### Metrik Root Mean Squared Error (RMSE)
"""

plt.plot(history.history['root_mean_squared_error'])
plt.plot(history.history['val_root_mean_squared_error'])
plt.title('model_metrics')
plt.ylabel('root_mean_squared_error')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

"""### Sistem Collaborative Filtering"""

book_data = books_new

# Mengambil sample user
user_id = data.UserID.sample(1).iloc[0]
book_visited_by_user = data[data.UserID == user_id]

# Operator bitwise (~), bisa diketahui di sini https://docs.python.org/3/reference/expressions.html
book_not_visited = book_data[~book_data['id'].isin(book_visited_by_user.ISBN.values)]['id']
book_not_visited = list(
    set(book_not_visited)
    .intersection(set(book_to_book_encoded.keys()))
)

book_not_visited = [[book_to_book_encoded.get(x)] for x in book_not_visited]
user_encoder = user_to_user_encoded.get(user_id)
user_book_array = np.hstack(
    ([[user_encoder]] * len(book_not_visited), book_not_visited))

"""### Hasil Sistem Rekomendasi Collaborative Filtering"""

ratings = model.predict(user_book_array).flatten()

top_ratings_indices = ratings.argsort()[-10:][::-1]
recommended_book_ids = [
    book_encoded_to_book.get(book_not_visited[x][0]) for x in top_ratings_indices
]

print('Menampilkan Rekomendasi Untuk Pengguna: {}'.format(user_id))
print('===' * 9)

print('10 Rekomendasi Buku Teratas')
print('----' * 8)

recommended_book = book_data[book_data['id'].isin(recommended_book_ids)]
i=1
for row in recommended_book.itertuples():
    print(i,row.title, ':', row.author)
    i+=1

"""### Mean Squared Error (MSE)"""

print("MSE dari pada data train = ", mean_squared_error(y_true=y_train, y_pred=model.predict(x_train))/1e3)
print("MSE dari pada data validation = ", mean_squared_error(y_true=y_val, y_pred=model.predict(x_val))/1e3)