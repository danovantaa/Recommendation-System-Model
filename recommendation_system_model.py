# -*- coding: utf-8 -*-
"""Recommendation System Model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jtFr9t18IaVxS8lzG9OtDM5EEhgFJnwL

# Laporan Proyek Machine Learning - Zefanya Danovanta Tarigan

## Project Overview

## Data Understanding

Dataset yang dipakai dalam proyek machine learning ini merupakan Book Recommendation Dataset dengan 271360 records data Books, 278858 records data Users dan 1149780 records data Ratings. Dataset ini bersifat open-source yang dipubilkasikan oleh MÖBIUS melalui platform Kaggle Topik dari datasetnya adalah Books yang berformat csv, Ratings yang berformat csv, dan User yang berformat csv.

**Tautan Dataset :** [Book Recommendation Dataset](https://www.kaggle.com/datasets/arashnic/book-recommendation-dataset)

Dataset yang pertama adalah **Users.csv** memiliki jumlah 278858 data dan 3 kolom, yakni :
1. `User-ID` : ID pengguna dari toko buku online
2. `Location`: lokasi pengguna.
3. `Age` : usia pengguna.

Dataset yang kedua yakni 'Books' yang memiliki jumlah 278858 data dan memiliki 8 kolom, diantaranya :
1. `ISBN`: identifikasi dari masing-masing buku.
2. `Book-Title`: judul buku.
3. `Book-Author`: penulis buku.
4. `Year-Of-Publication`: tahun dipublikasikannya buku.
5. `Publisher`: penerbit buku.
6. `Image-URL-S`: URL gambar cover buku dalam ukuran S(Small)
7. `Image-URL-M`: URL gambar cover buku dalam ukuran M(Medium)
8. `Image-URL-L`: URL gambar cover buku dalam ukuran L(Large)

Dataset yang ketiga yakni 'Ratings' yang memiliki jumlah 1149780 data dan memiliki 3 kolom, berikut penjelasan mengenai kolom-kolomnya :
Ratings.csv

1. `User-ID`: ID dari user yang memberikan rating terhadap buku.
2. `Kolom 'ISBN`: identifikasi buku atau nomor buku yang diberi rating oleh user
3. `Book-Rating`: nilai Rating dari buku, skala yang ada dalam rating ini yakni dari 0-10.

## Data Loading

### Menyiapkan Kredensial Kaggle
"""

# upload kaggle.json
from google.colab import files
files.upload()

!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json
!ls ~/.kaggle

# download dataset, choose 'copy api command' from kaggle dataset
!kaggle datasets download -d arashnic/book-recommendation-dataset

# unzip
!mkdir book-recommendation-dataset.zip
!unzip book-recommendation-dataset.zip -d book-recommendation-dataset
!ls book-recommendation-dataset

"""### Import Library"""

# Commented out IPython magic to ensure Python compatibility.
# Import library
import pandas as pd
import numpy as np
from zipfile import ZipFile
import tensorflow as tf
import seaborn as sns
import matplotlib.pyplot as plt
import keras
# %matplotlib inline
import seaborn as sns

from tensorflow import keras
from tensorflow.keras import layers
from pathlib import Path
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.metrics import precision_score
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from zipfile import ZipFile
from numpy import sqrt
from sklearn.metrics import mean_squared_error

# memuat file ke dalam variable
Books = pd.read_csv("/content/book-recommendation-dataset/Books.csv")
Ratings = pd.read_csv("/content/book-recommendation-dataset/Ratings.csv")
Users = pd.read_csv("/content/book-recommendation-dataset/Users.csv")

# menampilkan jumlah shape pada masing-masing file

print("Books Shape:", Books.shape)
print("Users Shape:", Users.shape)
print("Ratings Shape:", Ratings.shape)

Books.head()

"""## Exploratory Data Analysis

### Variabel `Books`
"""

#Cek informasi dari data Books
Books.info()

print('Banyak Buku :', len(Books))

data_books = Books.iloc[:12000]
print('Jumlah Buku yang di gunakan :', len(data_books))

"""### Variabel `Users`"""

Users.head()

Users.info()

print('Banyak Pengguna :', len(Users))

data_users = Users.iloc[:12000]
print('Jumlah pengguna yang di gunakan :', len(data_users))

"""### Variabel `Ratings`"""

Ratings.head()

Ratings.info()

print('Banyak Rating :', len(Ratings))

data_rating = Ratings.iloc[:12000]
print('Jumlah pengguna yang di gunakan :', len(data_rating))

"""## Data Preparation

### Variabel `Books`
"""

data_books.rename(columns = {'Book-Title':'Book_Title', 'Book-Author': 'Book_Author', 'Year-Of-Publication': 'Year_Publication'},inplace = True)
data_books.head()

data_books.drop(['Image-URL-S', 'Image-URL-M', 'Image-URL-L' ], axis=1, inplace=True)
data_books.head()

data_books['Year_Publication'] = pd.to_numeric(data_books['Year_Publication'], errors='coerce')

data_books.info()

"""### Variabel `Ratings`"""

data_rating.rename(columns = {'User-ID':'UserID', 'Book-Rating': 'Book_Rating'},inplace = True)
data_rating.head()

"""### Variabel `Users`"""

data_users.rename(columns = {'User-ID':'UserID'}, inplace = True)
data_users.head()

data_users.drop(['Age'], axis=1, inplace=True)
data_users.head()

print('Jumlah Buku berdasarkan Rating : ', len(data_rating.ISBN.unique()))
print('Jumlah Buku berdasarkan Daftar Buku : ', len(data_books.ISBN.unique()))
print('Jumlah Pengguna berdasarkan ID PEngguna : ', len(data_users.UserID.unique()))

data_train = data_rating.merge(data_books, left_on = 'ISBN', right_on = 'ISBN')
data_train.head()

data_train.info()

year=data_train['Year_Publication'].value_counts()[0:10]
plt.figure(figsize=(10,6))
plt.title("10 Tahun terbanyak publikasi", fontsize=20)
sns.barplot(x=year.index,y=year, palette = 'viridis')
plt.xlabel('Year',fontsize=14)
plt.ylabel('Count', fontsize=14)
plt.show()

data_using = data_rating.merge(data_users, left_on = 'UserID', right_on = 'UserID')
data_using.head()

data_using.info()

rating_counter=data_using['Book_Rating'].value_counts()[0:10]
plt.figure(figsize=(10,6))
plt.title("Jumlah Rating Buku yang Diberikan Pengguna", fontsize=20)
sns.barplot(x=rating_counter.index, y=rating_counter, palette='viridis')
plt.xlabel('Book_Rating', fontsize=14)
plt.ylabel('Jumlah Buku', fontsize=14)
plt.show()

most_author = data_train.Book_Author.value_counts().reset_index()
most_author.columns = ['Book_Author','count']

plt.figure(figsize = (10,6))
plt.title("10 Penulis Terpopuler",fontsize = 20)
sns.barplot(x = 'count', y = 'Book_Author', data = most_author.head(10), palette='viridis');
plt.ylabel('Book_Author', fontsize=14)
plt.xlabel('Count', fontsize=14)
plt.show()

most_loc = data_using.Location.value_counts().reset_index()
most_loc.columns = ['Location','count']

plt.figure(figsize = (10,6))
plt.title("Lokasi Penulis Terpopuler", fontsize=20)
sns.barplot(x = 'count', y = 'Location', data = most_loc.head(10), palette='viridis');
plt.ylabel('Location', fontsize=14)
plt.xlabel('Count', fontsize=14)
plt.show()

most_publis = data_train.Publisher.value_counts().reset_index()
most_publis.columns = ['Publisher','count']

plt.figure(figsize = (10,6))
plt.title("Publisher terbaik", fontsize= 20)
sns.barplot(x = 'count', y = 'Publisher', data = most_publis.head(10),  palette='viridis');
plt.ylabel('Publisher', fontsize=14)
plt.xlabel('Count', fontsize = 14)
plt.show()

data_aver = data_train.groupby('Book_Title', as_index=False)['Book_Rating'].mean()
temp = data_train.Book_Title.value_counts().reset_index()
temp.columns = ['Book_Title','count']
most_rated_by_reads = pd.merge(data_aver,temp,on='Book_Title')

most_rated_by_reads = most_rated_by_reads.sort_values('count',ascending=False)

plt.figure(figsize=(10,6))
plt.title("Rata-rata rating dengan buku terbanyak dibaca")
sns.barplot(x = 'Book_Rating', y = 'Book_Title', data = most_rated_by_reads.head(10), palette='viridis')
plt.xlabel('Book_Rating', fontsize=14)
plt.ylabel('Book_Title', fontsize=14)
plt.show()

"""## Data Cleaning"""

# check missing values pada data_books
(data_books.isnull() | data_books.empty | data_books.isna()).sum()

# check missing values pada data_rating
(data_rating.isnull() | data_rating.empty | data_rating.isna()).sum()

# check missing values pada data_users
(data_users.isnull() | data_users.empty | data_users.isna()).sum()

# check missing values pada data_train
(data_train.isnull() | data_train.empty | data_train.isna()).sum()

# check missing values pada data_using
(data_using.isnull() | data_using.empty | data_using.isna()).sum()

data_prep = data_train
data_prep.sort_values('ISBN').head()

data_prep = data_prep.drop_duplicates('ISBN')
data_prep.head()

data_prus = data_using
data_prus.sort_values('UserID').head()

data_prus = data_prus.drop_duplicates('UserID')
data_prus.head()

# Mengonversi data series 'ISBN’ menjadi dalam bentuk list
books_id = data_prep['ISBN'].tolist()

# Mengonversi data series ‘Title’ menjadi dalam bentuk list
books_title = data_prep['Book_Title'].tolist()

# Mengonversi data series ‘Author’ menjadi dalam bentuk list
books_author = data_prep['Book_Author'].tolist()

print('Jumlah ID Buku : ', len(books_id))
print('Jumlah Judul Buku : ', len(books_title))
print('Jumlah Penulis Buku : ', len(books_author))

# Membuat dictionary untuk data ‘books_id’, ‘books_title’, dan ‘books_author’
books_new = pd.DataFrame({
    'id': books_id,
    'title':books_title,
    'author': books_author
})
books_new.head()

data = data_rating
data.head()

# Mengubah UserID menjadi list tanpa nilai yang sama
user_ids = data['UserID'].unique().tolist()
print('list UserID: ', user_ids)

# Melakukan encoding userID
user_to_user_encoded = {x: i for i, x in enumerate(user_ids)}
print('encoded UserID : ', user_to_user_encoded)

# Melakukan proses encoding angka ke UserID
user_encoded_to_user = {i: x for i, x in enumerate(user_ids)}
print('encoded angka ke UserID: ', user_encoded_to_user)

# Mengubah ISBN menjadi list tanpa nilai yang sama
book_ids = data['ISBN'].unique().tolist()

# Melakukan proses encoding ISBN
book_to_book_encoded = {x: i for i, x in enumerate(book_ids)}

# Melakukan proses encoding angka ke ISBN
book_encoded_to_book = {i: x for i, x in enumerate(book_ids)}

#Selanjutnya, petakan userID dan ISBN ke dataframe yang berkaitan.

# Mapping userID ke dataframe user
data['user'] = data['UserID'].map(user_to_user_encoded)

# Mapping ISBN ke dataframe book
data['book'] = data['ISBN'].map(book_to_book_encoded)

# Mendapatkan jumlah user
num_users = len(user_to_user_encoded)
print(num_users)

# Mendapatkan jumlah book
num_book = len(book_encoded_to_book)
print(num_book)

# Mengubah rating menjadi nilai float
data['Book_Rating'] = data['Book_Rating'].values.astype(np.float32)

# Nilai minimum rating
min_rating = min(data['Book_Rating'])

# Nilai maksimal rating
max_rating = max(data['Book_Rating'])

print('Number of User: {}, Number of Resto: {}, Min Rating: {}, Max Rating: {}'.format(
    num_users, num_book, min_rating, max_rating
))

# Mengacak dataset
data = data.sample(frac=1, random_state=42)
data.head()

# Membuat variabel x untuk mencocokkan data user dan book menjadi satu value
x = data[['user', 'book']].values

# Membuat variabel y untuk membuat rating dari hasil
y = data['Book_Rating'].apply(lambda x: (x - min_rating) / (max_rating - min_rating)).values

# Membagi menjadi 90% data train dan 10% data validasi
train_indices = int(0.9 * data.shape[0])
x_train, x_val, y_train, y_val = (
    x[:train_indices],
    x[train_indices:],
    y[:train_indices],
    y[train_indices:]
)

print(x, y)

"""## Data Modeling"""

class RecommenderNet(tf.keras.Model):

  # Insialisasi fungsi
  def __init__(self, num_users, num_book, embedding_size, **kwargs):
    super(RecommenderNet, self).__init__(**kwargs)
    self.num_users = num_users
    self.num_book = num_book
    self.embedding_size = embedding_size
    self.user_embedding = layers.Embedding( # layer embedding user
        num_users,
        embedding_size,
        embeddings_initializer = 'he_normal',
        embeddings_regularizer = keras.regularizers.l2(1e-6)
    )
    self.user_bias = layers.Embedding(num_users, 1) # layer embedding user bias
    self.book_embedding = layers.Embedding( # layer embeddings book
        num_book,
        embedding_size,
        embeddings_initializer = 'he_normal',
        embeddings_regularizer = keras.regularizers.l2(1e-6)
    )
    self.book_bias = layers.Embedding(num_book, 1) # layer embedding book bias

  def call(self, inputs):
    user_vector = self.user_embedding(inputs[:,0]) # memanggil layer embedding 1
    user_bias = self.user_bias(inputs[:, 0]) # memanggil layer embedding 2
    book_vector = self.book_embedding(inputs[:, 1]) # memanggil layer embedding 3
    book_bias = self.book_bias(inputs[:, 1]) # memanggil layer embedding 4

    dot_user_book = tf.tensordot(user_vector, book_vector, 2)

    x = dot_user_book + user_bias + book_bias

    return tf.nn.sigmoid(x) # activation sigmoid

model = RecommenderNet(num_users, num_book, 50) # inisialisasi model

# model compile
model.compile(
    loss = tf.keras.losses.BinaryCrossentropy(),
    optimizer = keras.optimizers.Adam(learning_rate=0.001),
    metrics=[tf.keras.metrics.RootMeanSquaredError()]
)

# Memulai training

history = model.fit(
    x = x_train,
    y = y_train,
    batch_size = 64,
    epochs = 100,
    validation_data = (x_val, y_val)
)

"""## Evaluasi

### Metrik Root Mean Squared Error (RMSE)
"""

plt.plot(history.history['root_mean_squared_error'])
plt.plot(history.history['val_root_mean_squared_error'])
plt.title('model_metrics')
plt.ylabel('root_mean_squared_error')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

"""### Sistem Collaborative Filtering"""

book_data = books_new

# Mengambil sample user
user_id = data.UserID.sample(1).iloc[0]
book_visited_by_user = data[data.UserID == user_id]

# Operator bitwise (~), bisa diketahui di sini https://docs.python.org/3/reference/expressions.html
book_not_visited = book_data[~book_data['id'].isin(book_visited_by_user.ISBN.values)]['id']
book_not_visited = list(
    set(book_not_visited)
    .intersection(set(book_to_book_encoded.keys()))
)

book_not_visited = [[book_to_book_encoded.get(x)] for x in book_not_visited]
user_encoder = user_to_user_encoded.get(user_id)
user_book_array = np.hstack(
    ([[user_encoder]] * len(book_not_visited), book_not_visited))

"""### Hasil Sistem Rekomendasi Collaborative Filtering"""

ratings = model.predict(user_book_array).flatten()

top_ratings_indices = ratings.argsort()[-10:][::-1]
recommended_book_ids = [
    book_encoded_to_book.get(book_not_visited[x][0]) for x in top_ratings_indices
]

print('Menampilkan Rekomendasi Untuk Pengguna: {}'.format(user_id))
print('===' * 9)

print('10 Rekomendasi Buku Teratas')
print('----' * 8)

recommended_book = book_data[book_data['id'].isin(recommended_book_ids)]
i=1
for row in recommended_book.itertuples():
    print(i,row.title, ':', row.author)
    i+=1

"""### Mean Squared Error (MSE)"""

print("MSE dari pada data train = ", mean_squared_error(y_true=y_train, y_pred=model.predict(x_train))/1e3)
print("MSE dari pada data validation = ", mean_squared_error(y_true=y_val, y_pred=model.predict(x_val))/1e3)